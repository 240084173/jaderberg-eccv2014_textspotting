\documentclass[12pt]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[margin=2.5cm]{geometry}
\newcommand{\real}{\mathbb{R}}
\newcommand{\vv}{\operatorname{vec}}

\begin{document}

% ------------------------------------------------------------------
\section{Layers}
% ------------------------------------------------------------------

% ------------------------------------------------------------------
\subsection{Convolution}\label{s:convolution}
% ------------------------------------------------------------------

Let $X,Y$ denote the input and output images (maps) respectively. Let $F$ denote the filters. We assume that $X$ and $Y$ have been reshaped into two matrices, with the first index spanning spatial dimensions and the second spanning feature channels. Thus
\[
 X\in\real^{(wh) \times d}, \qquad
 Y\in\real^{(w'h') \times k}, \qquad
 F\in\real^{(w_fh_fd) \times k}
 \qquad
 w' = w - w_f + 1,
 \qquad
 h'' = h - h_f + 1,
\]
where $(w,h,d)$ is the size of the input image $X$, $(w',h',k)$ is the size of the output image, $(w_f,h_f,d)$ is the size of a filter, and there are $k$ filters. The output image $Y$ is a function of $X$ and $F$ and is connected to the output energy by a function $f$:
\[
Y = g(X,F),
\qquad
z = f(Y) \in \real.
\]
The operation $g$ is a linear filter, applying each of the filters in $F$ to produce each of the channels in $Y$. Up to a rearrangement of the elements of $X$, this can be written as a matrix multiplication. In particular, let $\phi(X)$ be the {\tt im2row} operator, which extracts from $X$ patches of the same volumes as the filters, placing them as columns of a matrix:
\[
 Y = g(X,F) = \phi(X) F,\qquad 
 \phi: \real^{(wh)\times d} \rightarrow \real^{(w'h')\times(w_fh_fd)}.
\]
Note that $\phi$ simply rearranges the elements of $X$ and is, therefore, a linear operator. In particular we can rewrite it as
\[
 \vv(\phi(X)) = H \vv(X), \qquad H \in \real^{(w'h'w_fh_fd) \times (whd)}
\]
for a suitable matrix $H$. The derivative of the function $f(g(X,F))$ are then given by
\[
\boxed{
\frac{dz}{dF}
=
\phi(X)^\top\frac{d f}{d Y},
\qquad
\frac{d z}{d \vv(X)}
=
H^\top
\vv\left(
\frac{d f}{d Y}F^\top
\right)
=
\phi^*\left(
\frac{d f}{d Y}F^\top
\right).
}
\]
Here we define the {\tt row2im} operator $H^\top$ as the dual of {\tt im2row}:
\[
 \vv(\phi^*(Y)) = H^\top \vv(Y).
\]
Let $(l,m,k,p,d)$ be an index in the {\tt im2row} output $\phi(X)$ and $(i,j,d)$ an index in the input $X$. Here indexes are mapped as $(l,m),(k,p,d)$ to the first and second index of $\phi(X)$ and to $(i,j),d$ of $X$. With slight abuse of notation one has:
\[
   [\phi(X)]_{(l,m,k,p,d)}= X_{(i,j,d)}, \qquad i=l+k,\quad j=m+p.
\]
Likewise for the dual operator {\tt row2im}:
\[
   [\phi^*(Y)]_{(i,j,d)} 
   = \sum_{k=0}^{w_f-1} \sum_{p=0}^{h_f-1} Y_{(i,j,k,p,d)}.
\]

\paragraph{Sizes, strides, and padding.}
Suppose we have $w$ pixels in the $x$ direction and a filter of size $w_f$. Then the filter is contained in the signal
\[
  w' = w - w_f + 1
\]
times (for all possible translations), provided that $w\geq w_f$. If the signal is padded with $p$ pixels to the left and to the right, then
\[
  w' = w + 2p - w_f + 1.
\]
If the filter output is subsampled every $\delta$ steps, then samples are at $i = \delta i'$. We must have
\[
0\leq i = \delta i' \leq w'-1
\qquad
\Rightarrow
\qquad
0 \leq i' \leq \lfloor \frac{w + 2p - w_f}{\delta} \rfloor.
\]


% ------------------------------------------------------------------
\subsection{Max pooling}\label{s:pooling}
% ------------------------------------------------------------------

Similarly to the convolution case, we define a function:
\[
 Y = g(X,\wedge), \quad z = f(Y) \in \real.
\]
Where
\[
\boxed{Y = g(X,\wedge) = \operatorname{maxrow}\phi(X).}
\]
and $\phi(X)$ is the {\tt im2row} operator defined above. In order to write more compact formulas for the derivative, we introduce the matrix $S(X) \in \real^{(w'h')
\times(w_fh_fd)}$ which selects the maximal element in each row of $\phi(X)$:
\[
  Y = \phi(X)S,
  \qquad
   S(X) = \operatornamewithlimits{argmax}_{S \geq 0,\ \mathbf{1}^\top S \leq \mathbf{1}^\top} \phi(X) S.
\]
Then the derivative is
\[
\boxed{
\frac{d z}{d \vv(X)}
=
H^\top
\vv\left(
\frac{d f}{d Y}S^\top
\right)
=
\phi^*\left(
\frac{d f}{d Y}S^\top
\right).
}
\]

% ------------------------------------------------------------------
\subsection{Normalization}\label{s:normalization}
% ------------------------------------------------------------------

The normalisation operation normalises the feature channels at any given spatial location $(i,j)$:
\[
 Y_{(i,j,k)} = X_{(i,j,k)} \left( \kappa + \alpha \sum_{t\in G(k)} X_{(i,j,t)}^2 \right)^{-\beta},
 \qquad
 z = f(Y),
\]
where $G(k) \subset \{1, 2, \dots, D\}$ is a subset of the input channels. Note that input $X$ and output $Y$ have the same dimensions. The derivative is easily computed as:
\[
\frac{dz}{d X_{(i,j,d)}}
=
\frac{dz}{d Y_{(i,j,d)}}
L(i,j,d|X)^{-\beta}
-2\alpha\beta
\sum_{k:d\in G(k)}
\frac{dz}{d Y_{(i,j,k)}}
L(i,j,k|X)^{-\beta-1} X_{(i,j,ki)} X_{(i,j,d)}
\]
where
\[
 L(i,j,k|X) = \kappa + \alpha \sum_{t\in G(k)} X_{(i,j,t)}^2.
\]

% ------------------------------------------------------------------
\subsection{Vectorisation}\label{s:vectorisation}
% ------------------------------------------------------------------

Vectorisation (utilised between convolutional and fully connected layers):
\[
 Y = \vv X, \qquad z = f(Y).
\]
The derivative is also a rearrangement of terms:
\[
\frac{dz}{dX} = \operatorname{reshape} \frac{dz}{dY}.
\]


% ------------------------------------------------------------------
\subsection{ReLU}\label{s:relu}
% ------------------------------------------------------------------

Rectified linear unit:
\[
 Y_k = \max\{0, X_k\}, \qquad z = f(Y).
\]
Derivative:
\[
\frac{dz}{dX_k}
=
\frac{dz}{dY_k} \delta_{\{X_k > 0 \}}.
\]

% ------------------------------------------------------------------
\subsection{Fully connected layer}\label{s:fully}
% ------------------------------------------------------------------

A fully connected layer is simply a matrix multiplication:
\[
  \vv Y = W \vv X, \qquad z = f(Y).
\]
The derivatives w.r.t. input $X$ and parameters $W$ are:
\[
\frac{dz}{d\vv(X)^\top}
= 
\frac{dz}{d(\vv Y)^\top} W,
\qquad
\frac{dz}{d W}
= 
\frac{df}{d \vv Y} (\vv X)^\top.
\]

% ------------------------------------------------------------------
\subsection{Softmax}\label{s:softmax}
% ------------------------------------------------------------------

Softmax:
\[
 Y_k = \frac{e^{X_i}}{\sum_{t=1}^D e^{X_t}}, \qquad z = f(Y).
\]
Derivative
\[
\frac{dz}{d X_d}
=
\sum_{k}
\frac{dz}{d Y_k}
\left(
e^{X_d} L(X)^{-1} \delta_{\{k=d\}}
-
e^{X_d}
e^{X_k} L(X)^{-2}
\right),
\quad
L(X) = \sum_{t=1}^D e^{X_t}.
\]
Simplifyng
\[
\frac{dz}{d X_d}
=
Y_d 
\left(
\frac{dz}{d Y_d}
-
\sum_{k=1}^K
\frac{dz}{d Y_k} Y_k.
\right).
\]

% ------------------------------------------------------------------
\subsection{Log-loss}\label{s:loss}
% ------------------------------------------------------------------

The log loss is:
\[
 y = \ell(X,c) = - \log X_c, \qquad z = f(y) = y,
\]
where $c \in \{1,2,\dots,D\}$ is the g.t\@. class of the image and, this being the output of the network, has $z=y$. The derivative is
\[
\frac{dz}{dX_c} = - \frac{1}{X_c} \delta_{\{k = c\}}.
\]
Note that one takes the average loss on all the training data.

% ------------------------------------------------------------------
\appendix\section{Proofs}\label{s:proofs}
% ------------------------------------------------------------------

\begin{align*}
\frac{d z}{d \vv(F)^\top}
&=
\frac{d f}{d \vv(Y)^\top}
\frac{d [\phi(X) F]}{d\vv(F)^\top}
\\
&=
\frac{d f}{d \vv(Y)^\top}
\frac{d [\left(
I_k \otimes \phi(X)
\right)
\vv(F)]
}{d\vv(F)^\top}
\\
&=
\vv\left(\frac{d f}{d Y} \right)^\top
\left(
I_k \otimes \phi(X)
\right)
\\
&=
\vv\left(
\phi(X)^\top I_k \frac{d f}{d Y} 
\right)^\top
\\
&=
\vv\left(
\phi(X)^\top
\frac{d f}{d Y} 
\right)^\top
\end{align*}
From which
\[
\frac{dz}{dF}
=
\phi(X)^\top\frac{d f}{d Y}.
\]
Also
\begin{align*}
\frac{d z}{d \vv(X)^\top}
&=
\frac{d f}{d \vv(Y)^\top}
\frac{d \vv [\phi(X) F]}{d\vv(X)^\top}
\\
&=
\frac{d f}{d \vv(Y)^\top}
\frac{d [(F^\top \otimes I_{w'h'})\vv(\phi(X))]}{d\vv(X)^\top}
\\
&=
\frac{d f}{d \vv(Y)^\top}
\frac{d [(F^\top \otimes I)H \vv(X)]}{d\vv(X)^\top}
\\
&=
\frac{d f}{d \vv(Y)^\top} (F^\top\otimes I)H
\\
&=
\vv\left(
\frac{d f}{d Y}F^\top
\right)^\top H
\end{align*}


\end{document}


